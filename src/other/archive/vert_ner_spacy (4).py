# -*- coding: utf-8 -*-
"""vert_ner_spacy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PYujLpStgUEMeddrOfGUTXogaPYJokB9
"""

# install spacy
#!pip install --upgrade spacy
#!pip install spacy-transformers -q
#!python -m spacy download en
#!python -m spacy download en_core_web_lg
#!python -m spacy download en_core_web_trf
#!-m pip install --upgrade transformers
#!python -m spacy download en_trf_bertbaseuncased_lg

import pandas as pd
import numpy as np
from spacy.training.example import Example
import spacy
from spacy.tokens import Doc, Token
from sklearn.model_selection import train_test_split
import random



data_path = '/content/dataset_short.csv'
df = pd.read_csv(data_path, delimiter=",",  error_bad_lines=False )
print(df.sample(5))
df.info()

sentences = df.groupby("sentence#")["word"].apply(list).values
labels = df.groupby(by = 'sentence#')['label'].apply(list).values

nlp = spacy.blank('en')

# Check if 'ner' pipeline exists, if not, add it
if 'ner' not in nlp.pipe_names:
    ner = nlp.add_pipe('ner', last=True)
else:
    ner = nlp.get_pipe('ner')

# Prepare training data
TRAIN_DATA = []
for sentence, label in zip(sentences, labels):
    doc = nlp.make_doc(' '.join(sentence))
    ents = [(word.idx, word.idx+len(word), lab) for word, lab in zip(doc, label) if lab != 'O']
    example = Example.from_dict(doc, {"entities": ents})
    TRAIN_DATA.append(example)

# Train model
optimizer = nlp.initialize()
for itn in range(100):
    random.shuffle(TRAIN_DATA)
    losses = {}
    for batch in spacy.util.minibatch(TRAIN_DATA, size=2):
        nlp.update(batch, sgd=optimizer, losses=losses)
    print(losses)

# Save model
nlp.to_disk("/path/to/model")


































