# -*- coding: utf-8 -*-
"""vert_ner_spacy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PYujLpStgUEMeddrOfGUTXogaPYJokB9
"""

# install spacy
#!pip install --upgrade spacy
#!pip install spacy-transformers -q
#!python -m spacy download en
#!python -m spacy download en_core_web_lg
#!python -m spacy download en_core_web_trf
#!-m pip install --upgrade transformers
#!python -m spacy download en_trf_bertbaseuncased_lg



import pandas as pd
import numpy as np
from spacy.training.example import Example
import spacy
from spacy.tokens import Doc, Token
import random



data_path = '/content/dataset_medium.csv'
df = pd.read_csv(data_path, delimiter=",",  error_bad_lines=False )
print(df.sample(5))
df.info()

nlp = spacy.load('en_core_web_trf')

training_data = []
for sentence_id, group in df.groupby('sentence#'):
    words = group['word'].tolist()
    labels = group['label'].tolist()

    # Convert words to a list of strings
    words = [str(word) for word in words]

    # Create a Doc object from the words
    doc = Doc(nlp.vocab, words=words)

    # Create a list of entities in BIO format
    entities = []
    start = 0
    for word, label in zip(words, labels):
        end = start + len(word)
        entities.append((start, end, doc.vocab.strings[label]))
        start = end + 1  # Assuming words are separated by a single space

    # Add the example to the training data
    example = Example.from_dict(doc, {"entities": entities})
    training_data.append(example)



# Fine-tune the pre-trained model
#nlp.disable_pipes('trf_wordpiecer', 'trf_tok2vec')
ner = nlp.get_pipe('ner')

#nlp.begin_training()

import time
# Update the model with your training data
for epoch in range(1):  # adjust the number of epochs as needed
    # Shuffle the training data
    spacy.util.fix_random_seed(1)
    random.shuffle(training_data)

    # Iterate over batches
    steps = 0
    for i, batch in enumerate(spacy.util.minibatch(training_data, size=32)):
        step_start = time.time()
        nlp.update(batch)
        step_end = time.time()
        step = step_end - step_start
        steps += step
        print("\r", f"{i:03d}/{len(training_data)//32:03d}; step: {step}; ETA: {(steps/i) * (len(training_data)//32-i)} seconds")
    print()

text_to_predict = "I am Ihor"
doc = nlp(text_to_predict)

for ent in doc.ents:
    print(f"Entity: {ent.text}, Label: {ent.label_}")