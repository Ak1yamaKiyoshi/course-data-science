{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ‚¨áÔ∏è Download dependencies"
      ],
      "metadata": {
        "id": "uHk0RJdD_EaP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "BDx8WLcUltJi"
      },
      "outputs": [],
      "source": [
        "# install spacy\n",
        "# !pip install -U spacy\n",
        "# !pip install --upgrade spacy\n",
        "# !pip install spacy-transformers -q\n",
        "# !-m pip install --upgrade transformers -q\n",
        "# !python -m spacy download en -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚¨ÖÔ∏è Importing libraries"
      ],
      "metadata": {
        "id": "RvALKwhJ98Fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from spacy.training.example import Example\n",
        "import spacy\n",
        "from spacy.tokens import Doc, Token\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import time"
      ],
      "metadata": {
        "id": "UOywhK0al084"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üì• Loading model"
      ],
      "metadata": {
        "id": "NK1Z_kaN-oO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pickle is the best option I could find. SpaCy.to_disk leaves behind a directory with a network that is difficult to import and export. It took me a long time to do this (using shutil, zip/unzip), so I can say that it is not the best option for the task."
      ],
      "metadata": {
        "id": "J8b6ZfuzYe2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('spacy_model.pkl', 'rb') as f:\n",
        "    nlp = pickle.load(f)"
      ],
      "metadata": {
        "id": "lhP2qnTFo_8o"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üéØ Accuracy on dataset"
      ],
      "metadata": {
        "id": "BWRJCt___W8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading a dataset as a pandas dataframe. Below you can see the columns.\n",
        "As for the structure of the dataset: each word is divided into sentences. By grouping the rows by sentence#, you can get sentences and labels for them.\n"
      ],
      "metadata": {
        "id": "6WOgahtzZZgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/dataset_medium.csv'\n",
        "df = pd.read_csv(data_path, delimiter=\",\",  error_bad_lines=False)\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McjqaJKn_cli",
        "outputId": "d3a3eefa-5d48-4ad4-b58c-e8fd0e3f771b"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 51640 entries, 0 to 51639\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   sentence#  51640 non-null  int64 \n",
            " 1   word       51640 non-null  object\n",
            " 2   label      51640 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 1.2+ MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-168-4f4cd120a22b>:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  df = pd.read_csv(data_path, delimiter=\",\",  error_bad_lines=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the model was trained on lowercase words, we translate the dataset to it, otherwise the results will be inaccurate."
      ],
      "metadata": {
        "id": "uID04lMJaWln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['word'] = df['word'].map(lambda x: x.lower())\n",
        "sentences = df.groupby(\"sentence#\")[\"word\"].apply(list).values\n",
        "labels = df.groupby(by = 'sentence#')['label'].apply(list).values"
      ],
      "metadata": {
        "id": "AROfHnJu_oXg"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To simplify the accuracy calculation, let's combine the words of the sentences with their labels."
      ],
      "metadata": {
        "id": "ik0P5kf2a0MJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = []\n",
        "for i in zip(sentences, labels):\n",
        "  dataset.append(list(zip(i[0], i[1]) ))"
      ],
      "metadata": {
        "id": "bjlj8f-N_ryQ"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy calculation on the dataset on which the model was trained. It is important to understand that the model cannot predict new words (mountain names) without context.\n",
        "The accuracy on the training dataset is demonstrated only to show how the model behaves on known and unknown data."
      ],
      "metadata": {
        "id": "BJ-bFPjKa2KI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_accuracy = 0\n",
        "actual_accuracy = 0\n",
        "for i, sentence in enumerate(sentences):\n",
        "    doc = nlp(\" \".join(sentence))\n",
        "    actual = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    prediction = [element for element in dataset[i] if element[1] == \"Mountain\"]\n",
        "\n",
        "    actual_accuracy += int(set(actual) == set(prediction))\n",
        "    total_accuracy += 1\n",
        "\n",
        "print(f\" accuracy: {round(actual_accuracy/total_accuracy, 2)*100}% \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Orfo8_dOEl__",
        "outputId": "efff2c4a-90ff-4f13-a78f-a0a864160b29"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " accuracy: 95.0% \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see, that model is well-trained."
      ],
      "metadata": {
        "id": "MU62AIVkb87F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚öôÔ∏è Predict entities"
      ],
      "metadata": {
        "id": "p6e95wBS-4an"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_entities(text, nlp):\n",
        "    doc = nlp(text.lower())\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "sentences_manual = [\n",
        "    \"I love climbing mountains.\",\n",
        "    \"Did you seen gora Hoverla pictures? They're awesome!\",\n",
        "    \"Hey, breakfast was good today\",\n",
        "    \"I like hora velikaya in Ukraine\",\n",
        "    \"I like velikaya mountain in Ukraine\"\n",
        "]\n",
        "for sentence in sentences_manual:\n",
        "    print(sentence)\n",
        "    print(predict_entities(sentence, nlp), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmQOH71N2_to",
        "outputId": "efe67314-cd46-4287-e794-12fc7c6ff570"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I love climbing mountains.\n",
            "[('i', 'Mountain')] \n",
            "\n",
            "Did you seen gora Hoverla pictures? They're awesome!\n",
            "[('hoverla', 'Mountain')] \n",
            "\n",
            "Hey, breakfast was good today\n",
            "[] \n",
            "\n",
            "I like hora velikaya in Ukraine\n",
            "[('i', 'Mountain'), ('velikaya', 'Mountain')] \n",
            "\n",
            "I like velikaya mountain in Ukraine\n",
            "[('i', 'Mountain')] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, I want to use two sentences:\n",
        "\n",
        "<br>\"I like hora velikaya in Ukraine\n",
        "<br>`[('i', 'Mountain'), ('velikaya', 'Mountain')]`\n",
        "\n",
        "<br>\"I like velikaya mountain in Ukraine\n",
        "<br>`[('i', 'Mountain')]`\n",
        "\n",
        "\n",
        "#### What is the difference here, and why in the second case the model did not recognize the Velikaya Mountain as a mountain?\n",
        "<br>\n",
        "In the dataset, namely, if we take into account the\n",
        "part where the names of Ukrainian mountains are found due to the specifics of the source, Ukrainian mountains have the word \"hora\" before the name, which corresponds to the English \"mountain\". Because of this and the attention mechanism in spacy transformers, the model \"knows\" that the word \"hora\" is used with Ukrainian mountain names. Therefore, despite the fact that the model could not recognize a large mountain by itself, using this context, the model still recognizes the mountain correctly.\n",
        "<br><br>\n",
        "This is a problem of the dataset alone. It should consist of many different examples, which is almost impossible to achieve in such a short period of time using chatgpt (primarily due to chatgpt limitations).\n",
        "<br><br>\n",
        "High-quality datasets are created by a team of people, checked and tested many times. This dataset is its 7th version.\n"
      ],
      "metadata": {
        "id": "L83FReJLc9dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print(\" \".join(sentences[random.randint(0, len(dataset))]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJ-BGVDaT6-3",
        "outputId": "6a9e2565-e720-4022-d9d1-201c4c50857b"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hora mulha and hora polovatynets celebrated the night away\n",
            "loma de los guananitos a remote hamlet preserves traditional customs\n",
            "phnum thma keh a region with historical charm\n",
            "mount tyree a rugged mountain challenging the brave\n",
            "cerro uran surrounded by lush vegetation was a haven\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuing with the example of the Ukrainian 'hora', other languages also have their own words. Since the dataset is compiled from the names of mountains in all countries, there are also 'cerro', 'gora', 'mountain', 'peak', and other similar words.\n",
        "\n",
        "They are all context that helps determine the label for words in a sentence."
      ],
      "metadata": {
        "id": "8L4d5Whrfq4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚öîÔ∏è Try it yourself!"
      ],
      "metadata": {
        "id": "Ae3eVnaaf9Pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "your_sentence = \"Did you seen hora Hoverla on photos? It's awesome!\"\n",
        "print(f\" <> your sentence: \\n - { your_sentence } \\n <> predicted labels: \\n{ predict_entities(your_sentence, nlp) }\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrwBruz4f2YK",
        "outputId": "78ce125b-5f7d-4056-ab83-c439dab3bfa3"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " <> your sentence: \n",
            " - Did you seen hora Hoverla on photos? It's awesome! \n",
            " <> predicted labels: \n",
            "[('hoverla', 'Mountain')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚ö†Ô∏è Warning an conclusion\n",
        "The way the model behaves is dictated solely by the dataset. It should be bigger, more detailed, and more diverse.\n",
        "\n",
        "But even with such a small dataset, the model shows good results, even though it relies heavily on context.\n",
        "\n",
        "This can be corrected.  "
      ],
      "metadata": {
        "id": "StI1oQfOhlBA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üë§ Personal thoughts\n",
        "This assignment helped me to better understand transformers, how they work, and the nuances of forming a dataset. I learned about several ready-made transformer solutions such as SpaCy, Pytotch pretrained bert, and others.\n",
        "I think it was a valuable experience.\n",
        "In case of comments, I am open to criticism and will be glad to hear your thoughts on how to improve the model itself or dataset generation"
      ],
      "metadata": {
        "id": "5wXB3GzIiKOK"
      }
    }
  ]
}