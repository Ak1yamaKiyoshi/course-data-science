{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " output size: 3, input_size: 3 \n",
      " x[0]: [1 1] y[0]: 1, \n",
      " x.shape: (3, 2), y.shape: (3,)\n",
      " Wxh: [0. 0. 0.], Whh: [0. 0. 0.], \n",
      " Why: [0. 0. 0.], bh: [0. 0. 0.], by: [0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RNN at 0x7f0a29e69110>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNN:\n",
    "    def __init__(self, \n",
    "                 start_random=True, \n",
    "                 activation_function:str=\"relu\", \n",
    "                ):\n",
    "        \"\"\" \n",
    "        start_random:bool - if no weight, will take random floats as weights \n",
    "        activation:str - activation function from options: relu, softmax, sigmoid \n",
    "        custom_activation_function:function - custom activation function, if set, always will use it instead of predefined ones \n",
    "        \"\"\"\n",
    "        self.start_random = start_random\n",
    "        self.bias = np.zeros(2)\n",
    "        # Input to hidden, hidden to hidden, hidden to output\n",
    "        self.weights = np.random.random(3) if start_random else np.zeros(3)\n",
    "        \n",
    "        self.activation_functions = {\n",
    "            \"relu\":       self.relu, \n",
    "        #    \"leaky_relu\": self.leaky_relu,\n",
    "        #    \"softmax\":    self.softmax,\n",
    "        #    \"sigmoid\":    self.sigmoid,\n",
    "        #    \"tanh\":       np.tanh,\n",
    "        }\n",
    "        self.derrivatives = {\n",
    "            \"relu\":       self.relu_df,\n",
    "        #    \"leaky_relu\": self.leaky_relu_df,\n",
    "        #    \"softmax\":    self.softmax_df,\n",
    "        #    \"sigmoid\":    self.sigmoid_df,\n",
    "        #    \"tanh\":       self.tanh_df,\n",
    "        }\n",
    "        self.loss_functions = {\n",
    "        #    \"binary-crossentropy\":                     self.binary_crossentropy,\n",
    "        #    \"categorical-crossentropy\":                self.categorical_crossentropy,\n",
    "            \"mean-squared-error\":                      self.mean_squared_error,\n",
    "        #    \"mean-absolute-error\":                     self.mean_absolute_error,\n",
    "        #    \"connectionist-temporal-classification\":   self.connectionist_temporal_classification,\n",
    "        #    \"kullback-leiber-divergence\":              self.kullback_leiber_divergence,\n",
    "        }\n",
    "        self.loss_function_derrivatives = {\n",
    "        #    \"binary-crossentropy\":                     self.binary_crossentropy_df,\n",
    "        #    \"categorical-crossentropy\":                self.categorical_crossentropy_df,\n",
    "        #    \"mean-squared-error\":                      self.mean_squared_error_df,\n",
    "        #    \"mean-absolute-error\":                     self.mean_absolute_error_df,\n",
    "        #    \"connectionist-temporal-classification\":   self.connectionist_temporal_classification_df,\n",
    "        #    \"kullback-leiber-divergence\":              self.kullback_leiber_divergence_df,\n",
    "        }\n",
    "\n",
    "        # Todo: find a better way to choose loss functions, maybe, by getting atributes of class?\n",
    "\n",
    "        # Choose your pokemon! \n",
    "        \n",
    "        self.activation = self.activation_functions[activation_function]\n",
    "        \n",
    "    \"\"\" Setting values \"\"\"\n",
    "         \n",
    "    def fit(self, x, y, debug_print=False):\n",
    "        input_size = np.array(x).shape[0]\n",
    "        output_size = np.array(y).shape[0]\n",
    "        \n",
    "        if debug_print: \n",
    "            print(f\" output size: {output_size}, input_size: {input_size} \\n x[0]: {x[0]} y[0]: {y[0]}, \\n x.shape: {x.shape}, y.shape: {y.shape}\")\n",
    "        \n",
    "        func = np.zeros if self.start_random else np.random.random\n",
    "        \n",
    "        self.Wxh = func(input_size)\n",
    "        self.Whh = func(input_size)\n",
    "        self.Why = func(output_size)\n",
    "        \n",
    "        self.bh = np.zeros(input_size)\n",
    "        self.by = np.zeros(output_size)\n",
    "        \n",
    "        if debug_print: \n",
    "            print(f\" Wxh: {self.Wxh}, Whh: {self.Whh}, \\n Why: {self.Why}, bh: {self.bh}, by: {self.by}\")\n",
    "        \n",
    "        return self \n",
    "        \n",
    "    @property\n",
    "    def func(self):\n",
    "        return self.activation_functions.keys()\n",
    "    \n",
    "    def from_weights(self, weights): \n",
    "        \"\"\" sets weights from weights, returns self \"\"\"\n",
    "        self.weights = np.array(weights)\n",
    "        return self\n",
    "\n",
    "    def set_bias(self, bias):\n",
    "        \"\"\" sets bias from bias, retuns self \"\"\"\n",
    "        self.bias = np.array(bias)\n",
    "        return self \n",
    "    \n",
    "    \"\"\" Main methods \"\"\"\n",
    "    \n",
    "    def train(self, x, y, learning_rate, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            pass#out\n",
    "            raise Exception(\"Not implemented yet \") \n",
    "\n",
    "    \n",
    "    def forward_pass(self, x:np.ndarray, index:int = None, get_states=False) -> tuple:\n",
    "        if not index: index = len(x)\n",
    "        states:np.ndarray = np.zeros(index)\n",
    "        \n",
    "        hidden_to_hidden:float = 0\n",
    "        for i, value in enumerate(x):\n",
    "            input_to_hidden:float = self.activation(value * self.weights[0] + hidden_to_hidden + self.bias[0])\n",
    "            hidden_to_hidden = input_to_hidden * self.weights[1]\n",
    "            states[i] = hidden_to_hidden\n",
    "        hidden_to_output = input_to_hidden * self.weights[2] + self.bias[1]\n",
    "        \n",
    "        return (hidden_to_output, None if not get_states else states)\n",
    "    \n",
    "    \"\"\" Activation functions \"\"\"\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return x * (x > 0)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        raise Exception(\"Not implemented \")\n",
    "        return np.exp(x)/np.sum(np.exp(x))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x)) \n",
    "    \n",
    "    def leaky_relu(self, x, alpha = 0.1):\n",
    "        return max(x, alpha * x)\n",
    "    \n",
    "    \"\"\" Derrivatives of activation functions \"\"\"\n",
    "    \n",
    "    def relu_df(self, x):\n",
    "        return np.heaviside(x, 1)\n",
    "\n",
    "    def leaky_relu_df(self, x, alpha = 0.1):\n",
    "        dx = np.ones_like(x)\n",
    "        dx[x < 0] = alpha\n",
    "        return dx\n",
    "\n",
    "    def softmax_df(self, s):\n",
    "        raise Exception(\"Not implemented yet \")        \n",
    "    \n",
    "    def sigmoid_df(self, x):\n",
    "        raise Exception(\"Not implemented yet \") \n",
    "\n",
    "    def tanh_df(self, x):\n",
    "        raise Exception(\"Not implemented yet \") \n",
    "    \n",
    "    \"\"\" Loss functions \"\"\"\n",
    "\n",
    "    def binary_crossentropy():\n",
    "        raise Exception(\"Not implemented yet \") \n",
    "\n",
    "    def categorical_crossentropy():\n",
    "        raise Exception(\"Not implemented yet \") \n",
    "\n",
    "    def mean_squared_error():\n",
    "        raise Exception(\"Not implemented yet \") \n",
    "\n",
    "    def mean_absolute_error():\n",
    "        raise Exception(\"Not implemented yet \") \n",
    "\n",
    "    def connectionist_temporal_classification():\n",
    "        raise Exception(\"Not implemented yet \") \n",
    "\n",
    "    def kullback_leiber_divergence():\n",
    "        raise Exception(\"Not implemented yet \") \n",
    "\n",
    "\n",
    "#RNN(activation_function='relu').from_weights([1.8, -0.5, 1.1]).set_bias([0, 0]).predict([1, 0.5, 0.5], debug_print=True, save_states=True)\n",
    "#RNN(activation_function='relu').from_weights([1.8, -0.5, 1.1]).set_bias([0, 0]).forward_pass(np.array([1, 0.5, 0.5]), get_states=True)\n",
    "#RNN().fit(np.array([1.8, -0.5, 1.1]), np.array([1, 1, 1, 1]))\n",
    "RNN().fit(np.array([ [1, 1], [1, 1], [1, 1] ]), np.array([1, 1, 1]), debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[135], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m input_m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m,])\n\u001b[0;32m----> 2\u001b[0m weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(input_m\u001b[38;5;241m.\u001b[39mshape[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      4\u001b[0m result_m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([]) \n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n",
      "File \u001b[0;32mmtrand.pyx:1270\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.randn\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mmtrand.pyx:1431\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.standard_normal\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_common.pyx:636\u001b[0m, in \u001b[0;36mnumpy.random._common.cont\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
